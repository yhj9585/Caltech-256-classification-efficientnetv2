{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# GPU 디바이스 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 480\n",
    "batch_size = 8\n",
    "NUM_CLASSES = 257\n",
    "\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train class distribution: Counter({256: 661, 250: 640, 144: 638, 252: 348, 231: 286, 95: 228, 10: 222, 104: 216, 125: 193, 7: 185, 11: 173, 89: 169, 158: 167, 91: 161, 239: 161, 146: 161, 137: 154, 128: 152, 131: 152, 147: 139, 192: 139, 136: 125, 108: 125, 2: 121, 157: 119, 226: 118, 4: 118, 20: 114, 167: 112, 213: 111, 100: 110, 132: 109, 211: 109, 45: 106, 63: 105, 142: 104, 119: 104, 126: 102, 3: 102, 42: 99, 18: 99, 39: 99, 233: 98, 14: 98, 62: 98, 116: 97, 43: 97, 151: 96, 181: 96, 113: 96, 36: 96, 188: 95, 133: 95, 71: 94, 140: 94, 53: 94, 251: 93, 79: 93, 112: 93, 206: 92, 73: 92, 24: 91, 50: 91, 234: 91, 193: 90, 23: 90, 191: 90, 189: 90, 84: 90, 230: 90, 92: 90, 190: 89, 122: 89, 102: 89, 149: 89, 209: 89, 22: 88, 223: 88, 27: 88, 175: 88, 88: 88, 37: 88, 67: 88, 207: 87, 150: 87, 117: 86, 255: 86, 197: 86, 115: 86, 141: 86, 48: 85, 6: 85, 35: 85, 172: 85, 56: 85, 25: 85, 198: 84, 216: 84, 156: 84, 127: 84, 93: 83, 16: 83, 195: 83, 29: 83, 121: 82, 169: 82, 148: 82, 60: 82, 220: 82, 32: 82, 8: 82, 47: 82, 160: 82, 179: 82, 153: 82, 184: 82, 254: 82, 28: 82, 55: 82, 208: 82, 124: 81, 17: 81, 31: 81, 163: 81, 212: 81, 64: 81, 245: 81, 33: 80, 182: 80, 248: 80, 72: 80, 232: 80, 196: 80, 97: 80, 26: 80, 199: 80, 180: 80, 225: 79, 78: 79, 68: 79, 70: 79, 171: 79, 205: 78, 235: 78, 177: 78, 54: 78, 237: 78, 215: 78, 12: 78, 1: 78, 0: 78, 21: 78, 49: 78, 103: 78, 87: 78, 201: 78, 249: 77, 176: 77, 155: 77, 227: 76, 253: 76, 240: 76, 80: 76, 210: 76, 82: 76, 228: 76, 186: 76, 90: 76, 9: 75, 229: 75, 46: 75, 219: 75, 130: 74, 241: 74, 246: 74, 99: 74, 152: 74, 168: 74, 76: 74, 154: 74, 134: 74, 57: 74, 165: 74, 135: 74, 86: 74, 15: 73, 123: 73, 244: 73, 202: 73, 161: 73, 129: 73, 221: 73, 243: 73, 30: 72, 224: 72, 77: 72, 242: 72, 162: 72, 5: 72, 236: 72, 96: 71, 106: 71, 139: 71, 118: 70, 40: 70, 194: 70, 114: 70, 105: 70, 59: 70, 204: 70, 34: 70, 41: 70, 187: 70, 164: 70, 101: 70, 13: 69, 138: 69, 94: 69, 166: 69, 38: 68, 107: 68, 44: 68, 109: 68, 51: 68, 183: 67, 75: 67, 218: 67, 69: 67, 214: 67, 247: 67, 111: 67, 83: 67, 110: 67, 170: 67, 173: 67, 238: 67, 65: 66, 74: 66, 52: 66, 58: 66, 66: 66, 159: 66, 174: 66, 145: 66, 120: 66, 19: 66, 143: 66, 61: 66, 217: 65, 81: 65, 200: 65, 185: 65, 178: 64, 85: 64, 203: 64, 222: 64, 98: 64})\n",
      "Validation class distribution: Counter({256: 166, 144: 160, 250: 160, 252: 87, 231: 72, 95: 57, 10: 56, 104: 54, 125: 49, 7: 47, 89: 43, 11: 43, 158: 42, 146: 41, 91: 40, 239: 40, 137: 38, 128: 38, 131: 38, 147: 35, 192: 35, 136: 31, 108: 31, 4: 30, 157: 30, 2: 30, 226: 29, 213: 28, 100: 28, 167: 28, 20: 28, 211: 27, 132: 27, 45: 27, 126: 26, 63: 26, 142: 26, 119: 26, 3: 25, 42: 25, 18: 25, 39: 25, 36: 24, 151: 24, 62: 24, 233: 24, 181: 24, 188: 24, 14: 24, 133: 24, 116: 24, 113: 24, 53: 24, 43: 24, 71: 24, 140: 23, 73: 23, 251: 23, 112: 23, 79: 23, 206: 23, 234: 23, 50: 23, 24: 23, 37: 22, 23: 22, 207: 22, 149: 22, 67: 22, 150: 22, 255: 22, 117: 22, 175: 22, 84: 22, 191: 22, 122: 22, 193: 22, 27: 22, 230: 22, 102: 22, 209: 22, 88: 22, 190: 22, 223: 22, 189: 22, 22: 22, 92: 22, 197: 22, 172: 21, 93: 21, 115: 21, 29: 21, 254: 21, 195: 21, 216: 21, 25: 21, 6: 21, 16: 21, 56: 21, 198: 21, 28: 21, 160: 21, 141: 21, 48: 21, 121: 21, 156: 21, 153: 21, 148: 21, 127: 21, 184: 21, 220: 21, 47: 21, 35: 21, 208: 20, 180: 20, 248: 20, 60: 20, 177: 20, 171: 20, 97: 20, 70: 20, 55: 20, 72: 20, 245: 20, 87: 20, 31: 20, 124: 20, 26: 20, 169: 20, 212: 20, 205: 20, 17: 20, 33: 20, 68: 20, 196: 20, 232: 20, 182: 20, 163: 20, 54: 20, 215: 20, 179: 20, 225: 20, 78: 20, 64: 20, 0: 20, 32: 20, 199: 20, 12: 20, 8: 20, 155: 19, 237: 19, 57: 19, 253: 19, 82: 19, 152: 19, 1: 19, 46: 19, 86: 19, 235: 19, 186: 19, 90: 19, 210: 19, 229: 19, 176: 19, 228: 19, 249: 19, 227: 19, 21: 19, 80: 19, 103: 19, 135: 19, 240: 19, 9: 19, 134: 19, 201: 19, 241: 19, 49: 19, 219: 19, 99: 19, 96: 18, 244: 18, 106: 18, 246: 18, 139: 18, 221: 18, 165: 18, 30: 18, 130: 18, 101: 18, 123: 18, 187: 18, 77: 18, 129: 18, 236: 18, 15: 18, 162: 18, 224: 18, 161: 18, 114: 18, 243: 18, 202: 18, 76: 18, 164: 18, 5: 18, 168: 18, 242: 18, 154: 18, 75: 17, 44: 17, 107: 17, 159: 17, 173: 17, 105: 17, 238: 17, 110: 17, 214: 17, 58: 17, 118: 17, 38: 17, 13: 17, 19: 17, 74: 17, 40: 17, 66: 17, 170: 17, 166: 17, 34: 17, 69: 17, 41: 17, 247: 17, 218: 17, 174: 17, 183: 17, 109: 17, 83: 17, 94: 17, 204: 17, 59: 17, 61: 17, 111: 17, 194: 17, 51: 17, 65: 17, 138: 17, 145: 16, 81: 16, 203: 16, 143: 16, 178: 16, 200: 16, 85: 16, 217: 16, 185: 16, 222: 16, 98: 16, 52: 16, 120: 16})\n",
      "Number of classes: 257\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터셋 경로 설정\n",
    "dataset_path = './256_ObjectCategories'\n",
    "\n",
    "data_dir = dataset_path\n",
    "\n",
    "\n",
    "# 데이터 전처리 및 augmentation 설정\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(img_size),\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# ImageFolder를 사용하여 전체 데이터셋 불러오기\n",
    "#full_dataset = datasets.ImageFolder(data_dir, transform=data_transforms['train'])\n",
    "full_dataset = datasets.ImageFolder(data_dir)\n",
    "# 데이터셋 크기 확인\n",
    "dataset_size = len(full_dataset)\n",
    "indices = list(range(dataset_size))\n",
    "targets = [full_dataset.targets[i] for i in indices]\n",
    "\n",
    "# StratifiedShuffleSplit을 사용하여 데이터셋 분할\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_indices, val_indices = next(split.split(indices, targets))\n",
    "\n",
    "# Subset을 사용하여 데이터셋을 분할\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)\n",
    "\n",
    "train_dataset.dataset.transform = models.EfficientNet_V2_M_Weights.IMAGENET1K_V1.transforms()#data_transforms['train']\n",
    "val_dataset.dataset.transform = models.EfficientNet_V2_M_Weights.IMAGENET1K_V1.transforms()#data_transforms['val']\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 클래스 분포 확인\n",
    "train_classes = [full_dataset.targets[i] for i in train_indices]\n",
    "val_classes = [full_dataset.targets[i] for i in val_indices]\n",
    "\n",
    "train_class_distribution = Counter(train_classes)\n",
    "val_class_distribution = Counter(val_classes)\n",
    "\n",
    "print(f'Train class distribution: {train_class_distribution}')\n",
    "print(f'Validation class distribution: {val_class_distribution}')\n",
    "\n",
    "num_classes = len(train_class_distribution)\n",
    "print(f'Number of classes: {num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "model = models.efficientnet_v2_m(weights = models.EfficientNet_V2_M_Weights.IMAGENET1K_V1)\n",
    "model.classifier = nn.Linear(model.classifier[1].in_features, 512)\n",
    "model.classifier.add_module(\"dropout\", nn.Dropout(0.4))\n",
    "model.classifier.add_module(\"final_fc\", nn.Linear(512,num_classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class LabelSmoothingCrossEntropy(torch.nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        logprobs = F.log_softmax(pred, dim=-1)\n",
    "        nll_loss = -logprobs.gather(dim=-1, index=target.unsqueeze(1))\n",
    "        nll_loss = nll_loss.squeeze(1)\n",
    "        smooth_loss = -logprobs.mean(dim=-1)\n",
    "        loss = confidence * nll_loss + self.smoothing * smooth_loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_epochs = 1\n",
    "\n",
    "def lr_lambda(current_epoch):\n",
    "    if current_epoch < warmup_epochs:\n",
    "        return float(current_epoch) / float(warmup_epochs)\n",
    "    else:\n",
    "        return 0.5 * (1 + np.cos(np.pi * (current_epoch - warmup_epochs) / (epochs - warmup_epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "\n",
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "# RAdam, Adam Adagrad S(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-5)\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "#optimizer = optim.RAdam(model.parameters(), lr=0.0001)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True) # val loss 값이 2에폭 동안 작으면 다음 lr을 절반으로\n",
    "scaler = torch.cuda.amp.GradScaler()  # Mixed Precision Training을 위한 스케일러\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "#scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "\n",
    "#scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=50, T_mult=1, eta_max=0.001,  T_up=2, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1/100: 100%|██████████| 3061/3061 [10:19<00:00,  4.94it/s]\n",
      "Validation Epoch 1/100: 100%|██████████| 766/766 [00:45<00:00, 16.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 3.2350, Train Acc: 0.4281, Val Loss: 3.2811, Val Acc: 0.5224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2/100: 100%|██████████| 3061/3061 [09:34<00:00,  5.33it/s]\n",
      "Validation Epoch 2/100: 100%|██████████| 766/766 [00:42<00:00, 18.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Train Loss: 2.6145, Train Acc: 0.5690, Val Loss: 2.5978, Val Acc: 0.6380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3/100:  60%|█████▉    | 1833/3061 [05:46<03:52,  5.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     11\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     14\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\pytorchgpu\\lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss, correct = 0.0, 0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{epochs}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct.double() / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc.cpu().numpy())\n",
    "\n",
    "    model.eval()\n",
    "    running_loss, correct = 0.0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{epochs}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_acc = correct.double() / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_loss)\n",
    "    val_accuracies.append(epoch_acc.cpu().numpy())\n",
    "\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_losses[-1]:.4f}, Train Acc: {train_accuracies[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracies[-1]:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
